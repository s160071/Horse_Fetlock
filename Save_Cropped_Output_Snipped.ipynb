{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Save_Cropped.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu23QtGGqSvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is the classifier to determine whether the fetlock-joint has a bone fragment stuck.\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import glob\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
        "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax\n",
        "\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCV86Vamqf-3",
        "colab_type": "code",
        "outputId": "066d6f59-b8f1-4506-ba42-85c22814652e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Load the dataset as pickles\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "%cd /content/gdrive/My\\ Drive/Deep_Learning/\n",
        "with open(\"augmented_healthy_data.pickle\", 'rb') as handle:\n",
        "    healthy_samples = pickle.load(handle)\n",
        "\n",
        "with open(\"augmented_splinter_data.pickle\", 'rb') as handle:\n",
        "    splinter_samples = pickle.load(handle)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/Deep_Learning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ_Jh6QnqjkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resized_pictures_coord_to_sigmoid(x_y):\n",
        "  # To avoid predicting x and y outside of the frame. The output of the network will be squeezed between 0-1.\n",
        "  # In height Pixel 21 = 0 | Pixel 89 = 1\n",
        "  # In width  Pixel 21 = 0 | Pixel 44 = 1 \n",
        "  # This is because the input size is 110 x 65. And the crop is 42x42. (21 to from middle to each size)\n",
        "  \n",
        "  # new_metric = [0 1];\n",
        "  # height = [21 89];\n",
        "  # width = [21 44];\n",
        "  # coeffs_h = polyfit(height, new_metric, 1);\n",
        "  # coeffs_w = polyfit(width, new_metric, 1);\n",
        "  \n",
        "  return np.round(x_y * [0.0434782608695652, 0.0147058823529412] - [0.913043478260871, 0.308823529411765], 4)\n",
        "\n",
        "def squeezed_to_coords(x_y_norm):\n",
        "  # To go from 0 - 1 to the coodinate system\n",
        "  # h_coeffs = polyfit(new_metric, height, 1);\n",
        "  # w_coeffs = polyfit(new_metric, width, 1);\n",
        "\n",
        "  return np.round(x_y_norm * [23, 68] + [21, 21], 0).astype(int)\n",
        "\n",
        "\n",
        "def rgb2gray(rgb):\n",
        "  return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
        "\n",
        "def crop_pictures(large_pics, xy_coords):\n",
        "  n_samples = large_pics.shape[0]\n",
        "  cropped_array = np.ones(shape=(n_samples, 420, 420, 1)) # 3 if COLOR, 1 if NOT\n",
        "  for i, data in enumerate(zip(large_pics, xy_coords)):\n",
        "    pic_l = data[0]\n",
        "    xy_s = data[1]\n",
        "    x_rect_start = (xy_s-21)*10\n",
        "    x_rect_end = (xy_s+21)*10\n",
        "    cropped_array[i, :, :, :] = pic_l[x_rect_start[1]:x_rect_end[1], x_rect_start[0]:x_rect_end[0]]\n",
        "  return cropped_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Y7MUEmSqnKg",
        "colab_type": "code",
        "outputId": "4c9ff447-11f5-4b88-b3ba-fac9ab2d65de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# HEALTHY\n",
        "h_pictures_large = []\n",
        "h_pictures_small = []\n",
        "h_labels = []\n",
        "h_xy = []\n",
        "\n",
        "for data in zip(healthy_samples[:, 0], healthy_samples[:, 1], healthy_samples[:, 3]):\n",
        "  h_pictures_large.append(rgb2gray(data[0]).reshape(1100, 650, 1))\n",
        "  h_pictures_small.append(data[1])\n",
        "  #h_xy.append(data[3])\n",
        "  #train_labels.append(data[2])\n",
        "  if data[2] == 0:\n",
        "    h_labels.append([1, 0])\n",
        "  if data[2] == 1:\n",
        "    h_labels.append([0, 1])\n",
        "  \n",
        "h_pictures_large = np.array(h_pictures_large)\n",
        "h_pictures_small = np.array(h_pictures_small)\n",
        "h_labels = np.array(h_labels)\n",
        "\n",
        "print(\"Large Training picture shapes: \" + str(h_pictures_large.shape))\n",
        "print(\"Small Training picture shapes: \" + str(h_pictures_small.shape))\n",
        "print(\"Train labels shapes: \" + str(h_labels.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Large Training picture shapes: (332, 1100, 650, 1)\n",
            "Small Training picture shapes: (332, 110, 65, 1)\n",
            "Train labels shapes: (332, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns1I9n0EseQw",
        "colab_type": "code",
        "outputId": "7356b339-6d05-41d7-a49e-76159e368925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# SPLINTERED\n",
        "s_pictures_large = []\n",
        "s_pictures_small = []\n",
        "s_labels = []\n",
        "s_xy = []\n",
        "\n",
        "for data in zip(splinter_samples[:, 0], splinter_samples[:, 1], splinter_samples[:, 3]):\n",
        "  s_pictures_large.append(rgb2gray(data[0]).reshape(1100, 650, 1))\n",
        "  s_pictures_small.append(data[1])\n",
        "  #h_xy.append(data[3])\n",
        "  #train_labels.append(data[2])\n",
        "  if data[2] == 0:\n",
        "    s_labels.append([1, 0])\n",
        "  if data[2] == 1:\n",
        "    s_labels.append([0, 1])\n",
        "  \n",
        "s_pictures_large = np.array(s_pictures_large)\n",
        "s_pictures_small = np.array(s_pictures_small)\n",
        "s_labels = np.array(s_labels)\n",
        "\n",
        "print(\"Large Training picture shapes: \" + str(s_pictures_large.shape))\n",
        "print(\"Small Training picture shapes: \" + str(s_pictures_small.shape))\n",
        "print(\"Train labels shapes: \" + str(s_labels.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Large Training picture shapes: (324, 1100, 650, 1)\n",
            "Small Training picture shapes: (324, 110, 65, 1)\n",
            "Train labels shapes: (324, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBx3uJJSqyF5",
        "colab_type": "code",
        "outputId": "2bec7d4c-f60f-4689-c1e1-f150ae03ad94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "num_outputs = 2 # [X Y]\n",
        "input_channels = 1\n",
        "batch_size = 1\n",
        "\n",
        "Localizer = 0\n",
        "localizer = 0\n",
        "\n",
        "class Localizer(nn.Module):\n",
        "    def __init__(self, num_classes, batch_size):\n",
        "        super(Localizer, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "                \n",
        "        # Convolutional stuff\n",
        "        self.simple_conv1 = nn.Conv2d(in_channels=input_channels,\n",
        "                                     out_channels=32, \n",
        "                                     kernel_size=3, \n",
        "                                     padding=1,\n",
        "                                     stride = 2)\n",
        "        \n",
        "        self.simple_pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.simple_conv2 = nn.Conv2d(in_channels = 32,\n",
        "                                      out_channels= 16,\n",
        "                                      kernel_size = 3,\n",
        "                                      padding = 1)\n",
        "        \n",
        "        self.simple_conv3 = nn.Conv2d(in_channels = 16,\n",
        "                                      out_channels= 8,\n",
        "                                      kernel_size = 3,\n",
        "                                      padding = 1)\n",
        "\n",
        "      \n",
        "        self.linear1 = Linear(in_features=8*6*4,\n",
        "                            out_features=90)\n",
        "        self.linear2 = Linear(in_features=90,\n",
        "                             out_features=30)\n",
        "\n",
        "        self.l_out = Linear(in_features=30,\n",
        "                            out_features=num_outputs,\n",
        "                            bias=False)\n",
        "\n",
        "\n",
        "        # Activation\n",
        "        self.activation_relu = torch.nn.ReLU()\n",
        "        self.activation_sigmoid = torch.nn.Sigmoid()\n",
        "        \n",
        "        # Regularization\n",
        "        self.drop_layer_1 = torch.nn.Dropout(p=0.4)\n",
        "        self.drop_layer_2 = torch.nn.Dropout(p=0.3)\n",
        "        self.bn1 = torch.nn.BatchNorm2d(32)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(16)\n",
        "        self.bn3 = torch.nn.BatchNorm2d(8)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # Comes in B H W C (0, 1, 2, 3) comes out B C H W (0, 3, 1, 2)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "\n",
        "        x = self.simple_conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation_relu(x)\n",
        "        x = self.simple_pool(x)\n",
        "        \n",
        "        x = self.simple_conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.activation_relu(x)\n",
        "        x = self.simple_pool(x)\n",
        "        \n",
        "        x = self.simple_conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.activation_relu(x)\n",
        "        x = self.simple_pool(x)\n",
        "\n",
        "        x = x.view(-1, 8*6*4) # output dimensions from conv2 to a 1d\n",
        "        \n",
        "        x = self.linear1(x)\n",
        "        x = self.activation_relu(x)\n",
        "        x = self.drop_layer_1(x)\n",
        "        \n",
        "        x = self.linear2(x)\n",
        "        x = self.activation_relu(x)\n",
        "        x = self.drop_layer_2(x)\n",
        "      \n",
        "        return self.activation_sigmoid(self.l_out(x))\n",
        "    \n",
        "localizer = Localizer(num_outputs, batch_size)\n",
        "print(Localizer)\n",
        "\n",
        "path = F\"/content/gdrive/My Drive/Localizer_warmstarted.pt\" \n",
        "localizer.load_state_dict(torch.load(path), strict=True)\n",
        "\n",
        "localizer.float()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class '__main__.Localizer'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Localizer(\n",
              "  (simple_conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "  (simple_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (simple_conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (simple_conv3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (linear1): Linear(in_features=192, out_features=90, bias=True)\n",
              "  (linear2): Linear(in_features=90, out_features=30, bias=True)\n",
              "  (l_out): Linear(in_features=30, out_features=2, bias=False)\n",
              "  (activation_relu): ReLU()\n",
              "  (activation_sigmoid): Sigmoid()\n",
              "  (drop_layer_1): Dropout(p=0.4, inplace=False)\n",
              "  (drop_layer_2): Dropout(p=0.3, inplace=False)\n",
              "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (bn3): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b47JRBCKq0K5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a test with some small picture input\n",
        "\n",
        "localizer.eval()\n",
        "n = 0\n",
        "\n",
        "for n in range(len(h_pictures_small)):\n",
        "\n",
        "  X_SMALL = h_pictures_small[n].astype('float32')\n",
        "  X_SMALL = X_SMALL.reshape(batch_size, 110, 65, 1)\n",
        "  X_SMALL = Variable(torch.from_numpy(X_SMALL))\n",
        "\n",
        "\n",
        "  test_out = localizer(Variable(X_SMALL))\n",
        "  test_out = test_out.detach().numpy()\n",
        "  test_out = squeezed_to_coords(test_out)\n",
        "\n",
        "  X_BIG = h_pictures_large[n]\n",
        "  X_BIG = X_BIG.reshape(batch_size, 1100, 650, 1)\n",
        "  network_crop = crop_pictures(X_BIG, test_out)\n",
        "\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  fig.suptitle('Large pic | Predicted Zoom')\n",
        "  ax1.imshow(X_BIG.reshape(1100, 650), cmap=plt.get_cmap('gray'))\n",
        "  ax2.imshow(network_crop.reshape(420, 420), cmap=plt.get_cmap('gray'))\n",
        "  plt.show()\n",
        "\n",
        "  path = F\"/content/gdrive/My Drive/Deep_Learning/Images_Cropped\" \n",
        "  cv2.imwrite(\"Healthy_\"+str(n)+\".jpg\", network_crop[0, :, :, 0]*255)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKkzVGJ3zhzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a test with some small picture input\n",
        "\n",
        "localizer.eval()\n",
        "n = 0\n",
        "\n",
        "for n in range(len(s_pictures_small)):\n",
        "\n",
        "  X_SMALL = s_pictures_small[n].astype('float32')\n",
        "  X_SMALL = X_SMALL.reshape(batch_size, 110, 65, 1)\n",
        "  X_SMALL = Variable(torch.from_numpy(X_SMALL))\n",
        "\n",
        "\n",
        "  test_out = localizer(Variable(X_SMALL))\n",
        "  test_out = test_out.detach().numpy()\n",
        "  test_out = squeezed_to_coords(test_out)\n",
        "\n",
        "  X_BIG = s_pictures_large[n]\n",
        "  X_BIG = X_BIG.reshape(batch_size, 1100, 650, 1)\n",
        "  network_crop = crop_pictures(X_BIG, test_out)\n",
        "\n",
        "  '''\n",
        "  fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "  fig.suptitle('Large pic | Predicted Zoom')\n",
        "  ax1.imshow(X_BIG.reshape(1100, 650), cmap=plt.get_cmap('gray'))\n",
        "  ax2.imshow(network_crop.reshape(420, 420), cmap=plt.get_cmap('gray'))\n",
        "  plt.show()\n",
        "  '''\n",
        "  path = F\"/content/gdrive/My Drive/Deep_Learning/Images_Cropped/\" \n",
        "  cv2.imwrite(\"Splinter_\"+str(n)+\".jpg\", network_crop[0, :, :, 0]*255)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}